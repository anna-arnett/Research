{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6c30aa3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import cv2 \n",
    "import os\n",
    "import sys\n",
    "import torchvision.models as models\n",
    "from tqdm import tqdm\n",
    "\n",
    "#to import modules from py scripts\n",
    "module_path = os.path.abspath(os.path.join('.'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "from dataloader import NYUDatasetRGBD\n",
    "from bts import BTS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "62b6c672",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_DIR = '/scratch365/palarcon/Scene-Reconstruction/BTS/dataset/test_paths.txt'\n",
    "MODEL_DIR = '/scratch365/palarcon/Scene-Reconstruction/BTS/logs/checkpoints/model-49'\n",
    "TRAIN_DIR = '/scratch365/palarcon/Scene-Reconstruction/BTS/dataset/train_paths.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9015fb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#converts the depth_gt and depth_est from batch torchs of size [1, 1, H, W] to numpy arrays of size [H, W, 1]\n",
    "def tensor_to_numpy(depth_gt, depth_est):\n",
    "\n",
    "    #remove batch dimension using squeeze, detach from cuda, transfer to cpu and convert to numpy\n",
    "    depth_est = depth_est.squeeze(1).detach().cpu().numpy()\n",
    "    #convert from (1, H, W) to (H, W, 1)\n",
    "    depth_est = np.transpose(np.array(depth_est), (1, 2, 0)) \n",
    "\n",
    "    #process ground_truth\n",
    "    depth_gt = depth_gt.squeeze(1).detach().cpu().numpy()\n",
    "    #convert from (1, H, W) to (H, W, 1)\n",
    "    depth_gt = np.transpose(np.array(depth_gt), (1, 2, 0)) \n",
    "\n",
    "    print(f'depth_gt shape: {depth_gt.shape}')\n",
    "    print(f'depth_est shape: {depth_est.shape}')\n",
    "\n",
    "    return depth_gt, depth_est\n",
    "\n",
    "#takes as\"' input an array of depth_estimate and depth_ground_truth and saved them to\n",
    "#an inference folder\n",
    "def save_images(depth_estimate):\n",
    "    #first, get the rgb image name in order as they appear in the test_paths.txt file\n",
    "    image_names = []\n",
    "    with open(TEST_DIR, 'r') as f:\n",
    "        rgb_path, _, _ = f.readline().split()\n",
    "        img_name = rgb_path.split('/')[-1] #get the image name\n",
    "        image_names.append(img_name)\n",
    "\n",
    "    #create an inference folder to save the images (if one already does not exist)\n",
    "    est_dir = 'BTS/depth_estimates'\n",
    "    if not os.path.isfile(est_dir):\n",
    "        os.makedirs(est_dir, exist_ok=True)\n",
    "\n",
    "    #save the images\n",
    "    for index in range(len(depth_estimate)):\n",
    "        \n",
    "        est_filename = est_dir + '/est_' + image_names[index].split('_')[-1] + '.png'\n",
    "        est = depth_estimate[index] * 1000.0 #upscaling the depth values \n",
    "        est_uint16 = est.astype(np.uint16)\n",
    "\n",
    "        cv2.imwrite(est_filename, est_uint16)\n",
    "    \n",
    "\n",
    "def compute_errors(gt, pred):\n",
    "    thresh = np.maximum((gt / pred), (pred / gt))\n",
    "    d1 = (thresh < 1.25).mean()\n",
    "    d2 = (thresh < 1.25 ** 2).mean()\n",
    "    d3 = (thresh < 1.25 ** 3).mean()\n",
    "\n",
    "    rms = (gt - pred) ** 2\n",
    "    rms = np.sqrt(rms.mean())\n",
    "\n",
    "    log_rms = (np.log(gt) - np.log(pred)) ** 2\n",
    "    log_rms = np.sqrt(log_rms.mean())\n",
    "\n",
    "    abs_rel = np.mean(np.abs(gt - pred) / gt)\n",
    "    sq_rel = np.mean(((gt - pred) ** 2) / gt)\n",
    "\n",
    "    err = np.log(pred) - np.log(gt)\n",
    "    silog = np.sqrt(np.mean(err ** 2) - np.mean(err) ** 2) * 100\n",
    "\n",
    "    err = np.abs(np.log10(pred) - np.log10(gt))\n",
    "    log10 = np.mean(err)\n",
    "\n",
    "    return [silog, abs_rel, log10, rms, sq_rel, log_rms, d1, d2, d3]\n",
    "\n",
    "def eval(gt_depth, est_depth):\n",
    "\n",
    "    est_depth[ est_depth < 1e-3] = 1e-3\n",
    "    est_depth[ est_depth > 10] = 10\n",
    "    est_depth[np.isinf( est_depth )] = 10\n",
    "    est_depth[np.isnan( est_depth )] = 1e-3\n",
    "\n",
    "    valid_mask = np.logical_and(gt_depth > 1e-3, gt_depth  < 10)\n",
    "    eval_mask = np.zeros(valid_mask.shape)\n",
    "    eval_mask[45:471, 41:601] = 1\n",
    "    valid_mask = np.logical_and(valid_mask, eval_mask)\n",
    "    measures = compute_errors(est_depth[valid_mask], gt_depth [valid_mask])\n",
    "\n",
    "    return measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e353c4fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch Version:  1.11.0+cu113\n",
      "CUDA Version: 11.3\n",
      "Number of GPUS  2\n",
      "Properties of first CUDA:\n",
      " _CudaDeviceProperties(name='Quadro RTX 6000', major=7, minor=5, total_memory=22698MB, multi_processor_count=72)\n",
      "Properties of second CUDA:\n",
      " _CudaDeviceProperties(name='Quadro RTX 6000', major=7, minor=5, total_memory=22698MB, multi_processor_count=72)\n"
     ]
    }
   ],
   "source": [
    "print(\"Pytorch Version: \", torch.__version__)\n",
    "print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "print(\"Number of GPUS \", torch.cuda.device_count())\n",
    "print(\"Properties of first CUDA:\\n\",torch.cuda.get_device_properties(\"cuda:0\"))\n",
    "print(\"Properties of second CUDA:\\n\",torch.cuda.get_device_properties(\"cuda:1\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575339e0",
   "metadata": {},
   "source": [
    "# Inference "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ce2e0c",
   "metadata": {},
   "source": [
    "## Test Set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "642f6770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting depth for 654 images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 654/654 [00:41<00:00, 15.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "silog: 13.264090602348345\n",
      "abs_rel: 0.1255654334503622\n",
      "log10: 0.05695544405546985\n",
      "rms: 0.44651271390873903\n",
      "sq_rel: 0.08644735345468124\n",
      "log_rms: 0.16978069292295964\n",
      "d1: 0.8292802207235714\n",
      "d2: 0.9566749735684714\n",
      "d3: 0.9870880888733413\n",
      "\n",
      "Finished predicting depth for 654 images in 29.65840196609497 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "to_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "model = BTS(max_depth=10, bts_size=512)\n",
    "checkpoint = torch.load(MODEL_DIR, map_location=to_device)\n",
    "model.load_state_dict(checkpoint['model'] )\n",
    "#model.eval()\n",
    "model.to(to_device)\n",
    "\n",
    "dataset_test = NYUDatasetRGBD(TEST_DIR, mode='test', isCAML=True)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=1, shuffle=True, num_workers=1)\n",
    "\n",
    "print(f'Predicting depth for {len(dataset_test)} images...')\n",
    "\n",
    "metric_measures = [0,0,0,0,0,0,0,0,0] #[silog, abs_rel, log10, rms, sq_rel, log_rms, d1, d2, d3]\n",
    "inference_time = 0\n",
    "with torch.no_grad():\n",
    "    for step, sampled_batch in enumerate(tqdm(dataloader_test)):\n",
    "\n",
    "            \n",
    "        image = sampled_batch['image'].to(to_device)\n",
    "        depth_gt = sampled_batch['depth'].to(to_device)      \n",
    "        focal = sampled_batch['focal'].to(to_device)\n",
    "\n",
    "        start = time.time()\n",
    "        lpg8x8, lpg4x4, lpg2x2, reduc1x1, depth_est = model(image, focal)\n",
    "        end = time.time()\n",
    "        inference_time += (end-start)\n",
    "\n",
    "        depth_gt = depth_gt.cpu().numpy().squeeze()\n",
    "        depth_est = depth_est.cpu().numpy().squeeze()\n",
    "        measures = eval(depth_gt,depth_est)\n",
    "        metric_measures = np.add(measures, metric_measures)\n",
    "\n",
    "#printing results\n",
    "metric_name = ['silog', 'abs_rel', 'log10', 'rms', 'sq_rel', 'log_rms', 'd1', 'd2', 'd3']\n",
    "for i in range(len(metric_name)):\n",
    "    print(f'{metric_name[i]}: {metric_measures[i]/len(dataset_test)}')\n",
    "\n",
    "print(f'\\nFinished predicting depth for {len(dataset_test)} images in {inference_time} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c4b69a",
   "metadata": {},
   "source": [
    "## Train Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "93150b05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting depth for 24231 images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24231/24231 [25:04<00:00, 16.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "silog: 4.637116014368289\n",
      "abs_rel: 0.037437478810122\n",
      "log10: 0.016713953279666773\n",
      "rms: 0.14934275381792556\n",
      "sq_rel: 0.009874079478754324\n",
      "log_rms: 0.054195382717141576\n",
      "d1: 0.9853351189603794\n",
      "d2: 0.9967035479315943\n",
      "d3: 0.9990765996778405\n",
      "Finished predicting depth for 24231 images in 1084.2540822029114 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataset_train = NYUDatasetRGBD(TRAIN_DIR, mode='train', isCAML=True)\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=1, shuffle=True, num_workers=1)\n",
    "\n",
    "print(f'Predicting depth for {len(dataset_train)} images...')\n",
    "\n",
    "metric_measures = [0,0,0,0,0,0,0,0,0] #[silog, abs_rel, log10, rms, sq_rel, log_rms, d1, d2, d3]\n",
    "inference_time = 0\n",
    "with torch.no_grad():\n",
    "    for step, sampled_batch in enumerate(tqdm(dataloader_train)):\n",
    "\n",
    "        image = sampled_batch['image'].to(to_device)\n",
    "        depth_gt = sampled_batch['depth'].to(to_device)      \n",
    "        focal = sampled_batch['focal'].to(to_device)\n",
    "\n",
    "        start = time.time()\n",
    "        lpg8x8, lpg4x4, lpg2x2, reduc1x1, depth_est = model(image, focal)\n",
    "        end = time.time()\n",
    "        inference_time += (end-start)\n",
    "\n",
    "        depth_gt = depth_gt.cpu().numpy().squeeze()\n",
    "        depth_est = depth_est.cpu().numpy().squeeze()\n",
    "        measures = eval(depth_gt,depth_est)\n",
    "        metric_measures = np.add(measures, metric_measures)\n",
    "\n",
    "#printing results\n",
    "metric_name = ['silog', 'abs_rel', 'log10', 'rms', 'sq_rel', 'log_rms', 'd1', 'd2', 'd3']\n",
    "for i in range(len(metric_name)):\n",
    "    print(f'{metric_name[i]}: {metric_measures[i]/len(dataset_train)}')\n",
    "\n",
    "print(f'Finished predicting depth for {len(dataset_train)} images in {inference_time} seconds')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a32176",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bf5bd2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
